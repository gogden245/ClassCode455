{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Underfitting\n",
    "    - too simple to explain the data \n",
    "\n",
    "Optimal fit\n",
    "    - not perfect, but will predict new data decently well\n",
    "\n",
    "Overfitting\n",
    "    - too good to be true (100% accuracy in a decision tree, too filtered down)\n",
    "    - future data won't be accurate\n",
    "    - including features that don't really apply\n",
    "\n",
    "To avoid under/overfitting, we want to get rid of bad features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance Measures\n",
    "- VIF- tells us which overlap\n",
    "- p- \n",
    "- t- \n",
    "- Coef- \n",
    "\n",
    "- R2- \n",
    "- MAE- \n",
    "- RMSEA- \n",
    "- Accuracy-\n",
    "- Precision-\n",
    "- Recall-\n",
    "- F-\n",
    "\n",
    "- pearson-\n",
    "- ANOVA-\n",
    "- correlation- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def import_data(path, messages=True):\n",
    "  import pandas as pd\n",
    "  df = pd.read_csv(path)\n",
    "  if messages: print(df.shape)\n",
    "  return df\n",
    "        \n",
    "def bin_groups(df, features=[], cutoff=0.05, replace_with='Other', messages=True):\n",
    "  import pandas as pd\n",
    "  if len(features) == 0: features = df.columns\n",
    "  for feat in features:\n",
    "    if feat in df.columns:  # Make sure they don't accidentally enter a feature name that doesn't exist\n",
    "      if not pd.api.types.is_numeric_dtype(df[feat]):\n",
    "        other_list = df[feat].value_counts()[df[feat].value_counts() / df.shape[0] < cutoff].index\n",
    "        df.loc[df[feat].isin(other_list), feat] = replace_with\n",
    "        if messages and len(other_list) > 0: print(f'{feat} has been binned by setting {other_list.values} to {replace_with}')\n",
    "    else:\n",
    "      if messages: print(f'{feat} not found in the DataFrame provided. No binning performed')\n",
    "  return df\n",
    "        \n",
    "def missing_drop(df, label, row_thresh=0.7, col_thresh=0.9, drop_all=False):\n",
    "  df.dropna(axis='rows', subset=[label], inplace=True)\n",
    "  df.dropna(axis='columns', thresh=1, inplace=True)\n",
    "  df.dropna(axis='rows', thresh=1, inplace=True)\n",
    "  df.dropna(axis='columns', thresh=round(df.shape[0] * row_thresh), inplace=True)\n",
    "  df.dropna(axis='rows', thresh=round(df.shape[1] * col_thresh), inplace=True)\n",
    "  if drop_all: df.dropna(axis='rows', inplace=True)\n",
    "  return df\n",
    "  \n",
    "def Xandy(df, label):\n",
    "  import pandas as pd\n",
    "  y = df[label]\n",
    "  X = df.drop(columns=[label])\n",
    "  return X, y\n",
    "  \n",
    "def dummy_code(X):\n",
    "  import pandas as pd\n",
    "  X = pd.get_dummies(X, drop_first=True)\n",
    "  return X\n",
    "  \n",
    "def minmax(X):\n",
    "  import pandas as pd\n",
    "  from sklearn.preprocessing import MinMaxScaler\n",
    "  X = pd.DataFrame(MinMaxScaler().fit_transform(X.copy()), columns=X.columns, index=X.index)\n",
    "  return X\n",
    "        \n",
    "def impute_KNN(df, label, neighbors=5):\n",
    "  from sklearn.impute import KNNImputer\n",
    "  import pandas as pd\n",
    "  X, y = Xandy(df, label)\n",
    "  X = dummy_code(X.copy())\n",
    "  X = minmax(X.copy())\n",
    "  imp = KNNImputer(n_neighbors=neighbors, weights=\"uniform\")\n",
    "  X = pd.DataFrame(imp.fit_transform(X), columns=X.columns, index=X.index)\n",
    "  return X.merge(y, left_index=True, right_index=True)\n",
    "        \n",
    "def fit_cv_regression(df, k, label, repeat=True, algorithm='ensemble', random_state=1, messages=True):\n",
    "  from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
    "  import pandas as pd\n",
    "  from numpy import mean\n",
    "  X, y = Xandy(df, label)\n",
    "  X = dummy_code(X)\n",
    "  if repeat:  cv = RepeatedKFold(n_splits=k, n_repeats=5, random_state=12345)\n",
    "  else:       cv = KFold(n_splits=k, random_state=12345, shuffle=True)\n",
    "  if algorithm == 'linear':\n",
    "    from sklearn.linear_model import Ridge, LassoLars\n",
    "    model1 = Ridge(random_state=random_state)\n",
    "    model2 = LassoLars(random_state=random_state)\n",
    "    score1 = mean(cross_val_score(model1, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "    score2 = mean(cross_val_score(model2, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  elif algorithm == 'ensemble':\n",
    "    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "    model1 = RandomForestRegressor(random_state=random_state)\n",
    "    model2 = GradientBoostingRegressor(random_state=random_state)\n",
    "    score1 = mean(cross_val_score(model1, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "    score2 = mean(cross_val_score(model2, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  else:\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "    from sklearn.neighbors import KNeighborsRegressor\n",
    "    model1 = MLPRegressor(random_state=random_state, max_iter=10000)\n",
    "    model2 = KNeighborsRegressor()\n",
    "    score1 = mean(cross_val_score(model1, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "    score2 = mean(cross_val_score(model2, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "  if messages:\n",
    "    print('R2', '{: <25}'.format(type(model1).__name__), round(score1, 4))\n",
    "    print('R2', '{: <25}'.format(type(model2).__name__), round(score2, 4))\n",
    "  if score1 > score2: return model1.fit(X, y)\n",
    "  else:               return model2.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def bivariate(df, label, roundto=4):\n",
    "    import pandas as pd, numpy as np\n",
    "    from scipy import stats\n",
    "        \n",
    "    output_df = pd.DataFrame(columns=['unique', 'sign', 'r', 'ρ', 'τ'])\n",
    "        \n",
    "    for feature in df.columns:\n",
    "        if feature != label:                    # Don't need the relationship fo the label with itself\n",
    "            df_temp = df[[feature, label]]      # Make a version of the DataFrame with only the feature and label\n",
    "            df_temp = df_temp.dropna()          # Drop missing rows from that DataFrame\n",
    "            unique = df_temp[feature].nunique() # Knowing the n unique values will help determine which corr metrics to use\n",
    "\n",
    "        r, p = stats.pearsonr(df_temp[feature], df_temp[label])\n",
    "        rho, rp = stats.spearmanr(df_temp[feature], df_temp[label])\n",
    "        tau, tp = stats.kendalltau(df_temp[feature], df_temp[label])\n",
    "            \n",
    "        if r < 0: sign = '-'  # To compare feature importance, we must convert all negative r to positive\n",
    "        else:     sign = '+'  # Therefore, let's make another column to inidcate it's sign\n",
    "\n",
    "        # Insert a record for this feature into output_df\n",
    "        output_df.loc[feature] = [unique, sign, round(abs(r), roundto),  round(abs(rho), roundto), \n",
    "                                    round(abs(tau), roundto)]\n",
    "    return output_df.sort_values(by=['r'], ascending=False)\n",
    "\n",
    "    # Make a DataFrame to keep track of feature importance based on each technique we learn\n",
    "\n",
    "df_importance = bivariate(df, 'SalePrice')\n",
    "df_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Create a model based on the linear algorithm with the highest fit metric\n",
    "model = fit_cv_regression(df, 10, label, algorithm='linear', messages=False)\n",
    "\n",
    "# Store the coefficients of that model in a new DataFrame\n",
    "df_coef = pd.DataFrame({'Coefficients':model.coef_}, index=model.feature_names_in_)\n",
    "df_coef['sign'] = '+'           # Add a column to indicate sign; default to positive\n",
    "for coef in df_coef.itertuples(): # Set each coefficient to the abs() and store the sign\n",
    "    if coef[1] < 0:                 # If it's negative, change to positive and store sign\n",
    "        df_coef.at[coef[0], 'sign'] = '-'\n",
    "        df_coef.at[coef[0], 'Coefficients'] = coef[1] * -1\n",
    "\n",
    "df_coef.sort_values(by=['Coefficients'], inplace=True)  # Sort the results for the image\n",
    "plt.figure(figsize=(4,20))                              # Set the size of the image              \n",
    "\n",
    "# Plot the coefficients in a bar chart separately for positive vs negative with diff colors\n",
    "colors = {'+': 'mediumseagreen', '-': 'lightcoral'}\n",
    "df_coef['Coefficients'].plot(kind='barh', color=[colors[i] for i in df_coef['sign']])\n",
    "\n",
    "# Create the plot legend\n",
    "labels = df_coef['sign'].unique()\n",
    "handles = [plt.Rectangle((0,0),1,1, color=colors[l]) for l in labels]\n",
    "plt.legend(handles, labels, title=\"Sign\")\n",
    "plt.show()\n",
    "\n",
    "# Store the coefficients in our df_importance table for later comparison\n",
    "df_importance = df_importance.merge(df_coef, left_index=True, right_index=True, suffixes=('', '_coef'))\n",
    "df_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "model = fit_cv_regression(df, 5, label, repeat=False, algorithm='ensemble')\n",
    "\n",
    "fi = pd.DataFrame(model.feature_importances_, columns=['FI'], index=model.feature_names_in_)\n",
    "df_importance = df_importance.merge(fi, left_index=True, right_index=True)\n",
    "df_importance.sort_values(by=['FI'], ascending=False, inplace=True)\n",
    "df_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "X, y = Xandy(df, label)\n",
    "result = permutation_importance(model, X, y, n_repeats=10, scoring=\"r2\", random_state=1, n_jobs=-1)\n",
    "df_pfi = pd.DataFrame({'PFI':result.importances_mean, 'PFI std':result.importances_std}, \n",
    "                    index=model.feature_names_in_)\n",
    "df_importance = df_importance.merge(df_pfi, left_index=True, right_index=True)\n",
    "df_importance.sort_values(by=['PFI'], ascending=False, inplace=True)\n",
    "df_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MULTIVARIATE FEATURE IMPORTANCE SCORES\n",
    "- takes into account other aspects, while correlation features (like r [pearson correlation-normal bell curve], p [spearman-rho - normal, but integers only], t[tau - not normal]) are isolated\n",
    "- more accurate than bi-variate models\n",
    "- types:\n",
    "\n",
    "    Coefficients-\n",
    "        - BEST in terms of speed\n",
    "        - GOOD in terms of accuracy\n",
    "        - linear\n",
    "\n",
    "    FI-\n",
    "        - BETTER in terms of speed\n",
    "        - BETTER in terms of accuracy\n",
    "        - decision tree based model\n",
    "        - if FI score is .53, it's 53% likely to show up highest in 100 (whatever number) decision trees\n",
    "\n",
    "    PFI\n",
    "        - GOOD in terms of speed\n",
    "        - BEST in terms of accuracy\n",
    "        - the portion of R-squared that this variable makes up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def fit_cv_classification_expanded(df, label, k=10, r=5, repeat=True, random_state=1):\n",
    "  import sklearn.linear_model as lm, pandas as pd, sklearn.ensemble as se, numpy as np\n",
    "  from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
    "  from numpy import mean, std\n",
    "  from sklearn import svm\n",
    "  from sklearn import gaussian_process\n",
    "  from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "  from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "  from sklearn import svm\n",
    "  from sklearn.naive_bayes import CategoricalNB\n",
    "  from xgboost import XGBClassifier\n",
    "  from sklearn import preprocessing\n",
    "  from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "  X = df.drop(columns=[label])\n",
    "  y = df[label]\n",
    "\n",
    "  if repeat:\n",
    "    cv = RepeatedKFold(n_splits=k, n_repeats=r, random_state=random_state)\n",
    "  else:\n",
    "    cv = KFold(n_splits=k, random_state=random_state, shuffle=True)\n",
    "  \n",
    "  fit = {}    # Use this to store each of the fit metrics\n",
    "  models = {} # Use this to store each of the models\n",
    "  \n",
    "  # Create the model objects\n",
    "  model_log = lm.LogisticRegression(max_iter=100)\n",
    "  model_logcv = lm.RidgeClassifier()\n",
    "  model_sgd = lm.SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "  model_pa = lm.PassiveAggressiveClassifier(max_iter=1000, random_state=random_state, tol=1e-3)\n",
    "  model_per = lm.Perceptron(fit_intercept=False, max_iter=10, tol=None, shuffle=False)\n",
    "  model_knn = KNeighborsClassifier(n_neighbors=3)\n",
    "  model_svm = svm.SVC(decision_function_shape='ovo') # Remove the parameter for two-class model\n",
    "  model_nb = CategoricalNB()\n",
    "  model_bag = se.BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, max_features=0.5)\n",
    "  model_ada = se.AdaBoostClassifier(n_estimators=100, random_state=random_state)\n",
    "  model_ext = se.ExtraTreesClassifier(n_estimators=100, random_state=random_state)\n",
    "  model_rf = se.RandomForestClassifier(n_estimators=10)\n",
    "  model_hgb = se.HistGradientBoostingClassifier(max_iter=100)\n",
    "  model_vot = se.VotingClassifier(estimators=[('lr', model_log), ('rf', model_ext), ('gnb', model_hgb)], voting='hard')\n",
    "  model_gb = se.GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "  estimators = [('ridge', lm.RidgeCV()), ('lasso', lm.LassoCV(random_state=random_state)), ('knr', KNeighborsRegressor(n_neighbors=20, metric='euclidean'))]\n",
    "  final_estimator = se.GradientBoostingRegressor(n_estimators=25, subsample=0.5, min_samples_leaf=25, max_features=1, random_state=random_state)\n",
    "  model_st = se.StackingRegressor(estimators=estimators, final_estimator=final_estimator)\n",
    "  model_xgb = XGBClassifier()\n",
    "  model_nn = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=random_state)\n",
    "\n",
    "  # Fit a cross-validated R squared score and add it to the dict\n",
    "  fit['Logistic'] = mean(cross_val_score(model_log, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['Ridge'] = mean(cross_val_score(model_logcv, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['SGD'] = mean(cross_val_score(model_sgd, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['PassiveAggressive'] = mean(cross_val_score(model_pa, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['Perceptron'] = mean(cross_val_score(model_per, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['KNN'] = mean(cross_val_score(model_knn, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['SVM'] = mean(cross_val_score(model_svm, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['NaiveBayes'] = mean(cross_val_score(model_nb, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['Bagging'] = mean(cross_val_score(model_bag, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['AdaBoost'] = mean(cross_val_score(model_ada, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['ExtraTrees'] = mean(cross_val_score(model_ext, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['RandomForest'] = mean(cross_val_score(model_rf, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['HistGradient'] = mean(cross_val_score(model_hgb, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['Voting'] = mean(cross_val_score(model_vot, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['GradBoost'] = mean(cross_val_score(model_gb, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['XGBoost'] = mean(cross_val_score(model_xgb, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  fit['NeuralN'] = mean(cross_val_score(model_nn, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "  \n",
    "  # XGBoost needs to LabelEncode the y before fitting the model\n",
    "  from sklearn.preprocessing import LabelEncoder\n",
    "  le = LabelEncoder().fit(y)\n",
    "  y_encoded = le.transform(y.copy())\n",
    "  fit['XGBoost'] = mean(cross_val_score(model_xgb, X, y_encoded, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "\n",
    "  # Add the model to another dictionary; make sure the keys have the same names as the list above\n",
    "  models['Logistic'] = model_log\n",
    "  models['Ridge'] = model_logcv\n",
    "  models['SGD'] = model_sgd\n",
    "  models['PassiveAggressive'] = model_pa\n",
    "  models['Perceptron'] = model_per\n",
    "  models['KNN'] = model_knn\n",
    "  models['SVM'] = model_svm\n",
    "  models['NaiveBayes'] = model_nb\n",
    "  models['Bagging'] = model_bag\n",
    "  models['AdaBoost'] = model_ada\n",
    "  models['ExtraTrees'] = model_ext\n",
    "  models['RandomForest'] = model_rf\n",
    "  models['HistGradient'] = model_hgb\n",
    "  models['Voting'] = model_vot\n",
    "  models['GradBoost'] = model_gb\n",
    "  models['XGBoost'] = model_xgb\n",
    "  models['NeuralN'] = model_nn\n",
    "\n",
    "  # Add the fit dictionary to a new DataFrame, sort, extract the top row, use it to retrieve the model object from the models dictionary\n",
    "  df_fit = pd.DataFrame({'Accuracy':fit})\n",
    "  df_fit.sort_values(by=['Accuracy'], ascending=False, inplace=True)\n",
    "  best_model = df_fit.index[0]\n",
    "  print(df_fit)\n",
    "\n",
    "  return models[best_model].fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Setting the label here since it is used in multiple function calls\n",
    "label = 'SalePrice'\n",
    "\n",
    "# Import the data\n",
    "df = import_data('housing.csv', messages=False)\n",
    "\n",
    "# Clean/prepare the data\n",
    "df = bin_groups(df, messages=False)\n",
    "df = missing_drop(df, label)\n",
    "df = impute_KNN(df, label)\n",
    "df.head()\n",
    "\n",
    "# Fit the model based on the algorithm with the best fit metrics\n",
    "print(f'Model fit before feature selection:')\n",
    "model = fit_cv_regression(df, 5, label)\n",
    "\n",
    "# Select the best features based on the model that was stored above\n",
    "df_reduced = select_features(df.copy(), label, model)\n",
    "\n",
    "# Print out the number of features as a sanity check\n",
    "print(f'\\nShape after feature selection:\\t{df_reduced.shape}\\n')\n",
    "\n",
    "# Retrain the model with the reduced feature set\n",
    "print(f'Model fit after feature selection:')\n",
    "model = fit_cv_regression(df_reduced, 5, label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
